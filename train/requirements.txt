# PyTorch — tensor operations and GPU acceleration
torch>=2.3

# Transformers — model architecture, tokeniser, and TrainingArguments
transformers>=5.0.0

# TRL (Transformer Reinforcement Learning) — SFTTrainer and DataCollatorForCompletionOnlyLM
trl>=0.9,<0.13

# PEFT — LoRA adapter configuration and parameter-efficient fine-tuning
peft>=0.12

# BitsAndBytes — 4-bit NF4 quantisation (QLoRA)
bitsandbytes>=0.43

# Datasets — streaming dataset loading from HF Hub
datasets>=2.20

# Accelerate — distributed training backend used internally by Trainer
accelerate>=0.32

# WandB — experiment tracking (loss curves, LR schedule, sample output tables)
wandb>=0.17

# HuggingFace Hub — push checkpoints and final adapter weights to HF Hub
huggingface_hub>=0.23

# ONNX runtime libraries are optional for local validation/inference tooling.
# The training script can spawn an isolated uv exporter env for ONNX export,
# so optimum-onnx is intentionally not pinned in this TF5 training env.
onnxruntime
onnx>=1.16
