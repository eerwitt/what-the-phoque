# PyTorch — tensor operations and GPU acceleration
torch>=2.3

# Transformers — model architecture, tokeniser, and TrainingArguments
transformers>=5.0.0

# TRL (Transformer Reinforcement Learning) — SFTTrainer and DataCollatorForCompletionOnlyLM
trl>=0.9,<0.13

# PEFT — LoRA adapter configuration and parameter-efficient fine-tuning
peft>=0.12

# BitsAndBytes — 4-bit NF4 quantisation (QLoRA)
bitsandbytes>=0.43

# Datasets — streaming dataset loading from HF Hub
datasets>=2.20

# Accelerate — distributed training backend used internally by Trainer
accelerate>=0.32

# WandB — experiment tracking (loss curves, LR schedule, sample output tables)
wandb>=0.17

# HuggingFace Hub — push checkpoints and final adapter weights to HF Hub
huggingface_hub>=0.23

# ONNX runtime libraries are optional for local validation/inference tooling.
# For standalone ONNX export, use train/requirements_export_onnx.txt
# (TF4-compatible export stack with optimum-onnx pinned).
onnxruntime
onnx>=1.16

# Visualization dependencies (train/visualize_runs.py)
matplotlib>=3.8
seaborn>=0.13
scikit-learn>=1.4
numpy>=1.26
